# Orange UI

Offline desktop chat client for GGUF LLMs using llama.cpp.

## Features
- Local GGUF model support
- Streaming responses
- Chat sessions (ChatGPT-style)
- System prompt
- CPU-first, offline by default

## Requirements
- Python 3.10+
- PySide6
- llama-cpp-python

## Run
```bash
python main.py
